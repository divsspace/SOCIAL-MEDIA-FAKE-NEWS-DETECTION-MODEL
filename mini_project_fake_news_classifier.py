# -*- coding: utf-8 -*-
"""MINI PROJECT-FAKE NEWS CLASSIFIER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gq04zMl9AAA9-wJcljLFesaY4bybmdvE

# PREPROCESSING
"""

import os
import re
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
import torch
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from textblob import TextBlob
from google.colab import drive

drive.mount('/content/drive')

dataset_path_15 = "/content/drive/MyDrive/archive2/twitter15"
dataset_path_16 = "/content/drive/MyDrive/archive2/twitter16"

label_mapping = {"true": 0, "false": 1, "unverified": 2, "non-rumor": 3}

#cleaning tweets, will be saved for future use
def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", "", text)  #remove urls
    text = re.sub(r"@\w+", "", text)  #remove mentions
    text = re.sub(r"#\w+", "", text)  #remove hashtags
    text = re.sub(r"[^a-zA-Z0-9\s]", "", text)  #remove special characters
    return text.strip()

def load_data(dataset_path): #dataset loading
    label_path = os.path.join(dataset_path, "label.txt")
    tweets_path = os.path.join(dataset_path, "source_tweets.txt")

    labels = {}
    tweets = {}

    with open(label_path, "r", encoding="utf-8") as f:
        for line in f:
            label, tweet_id = line.strip().split(":")
            if label in label_mapping:
                labels[tweet_id] = label_mapping[label]

    with open(tweets_path, "r", encoding="utf-8") as f:
        for line in f:
            parts = line.strip().split("\t", 1)
            if len(parts) == 2:
                tweet_id, text = parts
                cleaned_text = clean_text(text)
                if tweet_id in labels and cleaned_text:
                    tweets[tweet_id] = cleaned_text

    return tweets, labels

tweets_15, labels_15 = load_data(dataset_path_15)
tweets_16, labels_16 = load_data(dataset_path_16)

tweets = {**tweets_15, **tweets_16}
labels = {**labels_15, **labels_16}

#feature engineering
vectorizer = TfidfVectorizer(max_features=500)
tweet_features = vectorizer.fit_transform(tweets.values()).toarray()

sentiment_scores = {tweet_id: TextBlob(text).sentiment.polarity for tweet_id, text in tweets.items()}

G = nx.Graph()

for i, (tweet_id, text) in enumerate(tweets.items()):
    G.add_node(tweet_id, node_type="Tweet", label=labels[tweet_id], text=text, features=tweet_features[i], sentiment=sentiment_scores[tweet_id])

similarity_matrix = cosine_similarity(tweet_features)
threshold = 0.5  #similarity threshold to create an edge, according to similar words found in tweets

for i, id1 in enumerate(tweets.keys()):
    for j, id2 in enumerate(tweets.keys()):
        if i != j and similarity_matrix[i, j] > threshold:
            G.add_edge(id1, id2, weight=similarity_matrix[i, j], relation="text_similarity")

def check_nodes_integrity():
    dataset_nodes = set(tweets.keys())
    graph_nodes = set(G.nodes)
    return dataset_nodes == graph_nodes

plt.figure(figsize=(15, 12))
pos = nx.spring_layout(G, seed=42, k=0.1)
nx.draw(G, pos, with_labels=False, node_size=10, node_color="blue", edge_color="gray", alpha=0.3)
plt.show()

print("Full graph visualization complete.")

#graph validation/correctness based on similar word interaction

def check_graph_correctness():
    dataset_nodes = set(tweets.keys())
    graph_nodes = set(G.nodes)
    missing_nodes = dataset_nodes - graph_nodes
    extra_nodes = graph_nodes - dataset_nodes

    #edges have valid nodes or not
    invalid_edges = [(u, v) for u, v in G.edges if u not in graph_nodes or v not in graph_nodes]

    print(f"Total nodes in dataset: {len(dataset_nodes)}")
    print(f"Total nodes in graph: {len(graph_nodes)}")
    print(f"Missing nodes in graph: {len(missing_nodes)}")
    print(f"Extra nodes in graph: {len(extra_nodes)}")
    print(f"Total edges in graph: {len(G.edges)}")
    print(f"Invalid edges (edges with missing nodes): {len(invalid_edges)}")

    if len(missing_nodes) == 0 and len(extra_nodes) == 0 and len(invalid_edges) == 0:
        print("The graph is correctly built")
    else:
        print("Issues detected in the graph structure")

check_graph_correctness()
check_graph_correctness()

#preprocessed data saved for further use

import pickle

save_path = "/content/drive/MyDrive/archive2/preprocessed_data.pkl"

preprocessed_data = {
    "tweets": tweets,
    "labels": labels,
    "features": tweet_features,
    "sentiment_scores": sentiment_scores
}

with open(save_path, "wb") as f:
    pickle.dump(preprocessed_data, f)

print(f"Preprocessed data saved successfully at {save_path}")

#preprocessed data to verify cleaning of tweets
#special characters, emojis, other unwanted things removed

import pickle

save_path = "/content/drive/MyDrive/archive2/preprocessed_data.pkl"
with open(save_path, "rb") as f:
    preprocessed_data = pickle.load(f)

tweets = preprocessed_data["tweets"]
labels = preprocessed_data["labels"] #true, false, etc.
sentiments = preprocessed_data["sentiment_scores"]

for i, tweet_id in enumerate(list(tweets.keys())[:5]):
    print(f"Tweet ID: {tweet_id}")
    print(f"Cleaned Text: {tweets[tweet_id]}")
    print(f"Label: {labels[tweet_id]}")
    print("-" * 50)

#raw data for comparison
def load_raw_tweets(dataset_path):
    tweets_path = os.path.join(dataset_path, "source_tweets.txt")
    raw_tweets = {}

    with open(tweets_path, "r", encoding="utf-8") as f:
        for line in f:
            parts = line.strip().split("\t", 1)
            if len(parts) == 2:
                tweet_id, text = parts
                raw_tweets[tweet_id] = text

    return raw_tweets

raw_tweets_15 = load_raw_tweets(dataset_path_15)
raw_tweets_16 = load_raw_tweets(dataset_path_16)

raw_tweets = {**raw_tweets_15, **raw_tweets_16}

for i, (tweet_id, text) in enumerate(list(raw_tweets.items())[:5]):
    print(f"Tweet ID: {tweet_id}")
    print(f"Raw Text: {text}")
    print("-" * 50)

#data consistency, checking for missing labels and removing them
#no data loss
#earlier 2139, 1 label missing, after check: 2138

# Define tweet_ids, tweet_texts, tweet_labels before using them
tweet_ids = list(tweets.keys())
tweet_texts = list(tweets.values())
tweet_labels = list(labels.values())

print(f"Tweet IDs: {len(tweet_ids)}")
print(f"Tweet Texts: {len(tweet_texts)}")
print(f"Tweet Labels: {len(tweet_labels)}")
print(f"Tweet Features: {len(tweet_features)}")

id_set = set(tweet_ids)
label_set = set(labels.keys())

extra_labels = label_set - id_set
missing_labels = id_set - label_set

print(f"Extra labels (not in tweet IDs): {extra_labels}")
print(f"Missing labels (not in labels dict): {missing_labels}")

if extra_labels:
    for extra_id in extra_labels:
        del labels[extra_id]

tweet_labels = list(labels.values())

print(f"After Fixing - Tweet Labels: {len(tweet_labels)}")

import numpy as np
from sklearn.model_selection import train_test_split

tweet_ids = list(tweets.keys())
tweet_texts = list(tweets.values())
tweet_labels = list(labels.values())
tweet_features = np.array(tweet_features)

#split into 70% training, 30% to further split
train_ids, remaining_ids, train_texts, remaining_texts, train_labels, remaining_labels, train_features, remaining_features = train_test_split(
    tweet_ids, tweet_texts, tweet_labels, tweet_features, test_size=0.3, random_state=42, stratify=tweet_labels
)

#15% for testing, 15% for validation
val_ids, test_ids, val_texts, test_texts, val_labels, test_labels, val_features, test_features = train_test_split(
    remaining_ids, remaining_texts, remaining_labels, remaining_features, test_size=0.5, random_state=42, stratify=remaining_labels
)

print(f"Training Data: {len(train_ids)} tweets") #1496
print(f"Validation Data: {len(val_ids)} tweets") #321
print(f"Testing Data: {len(test_ids)} tweets") #321

#total = 2138

split_data = {
    "train": {"ids": train_ids, "texts": train_texts, "labels": train_labels, "features": train_features},
    "val": {"ids": val_ids, "texts": val_texts, "labels": val_labels, "features": val_features},
    "test": {"ids": test_ids, "texts": test_texts, "labels": test_labels, "features": test_features},
}

#saving split data
import pickle

save_path = "/content/drive/MyDrive/archive2/split_preprocessed_data.pkl"

with open(save_path, "wb") as f:
    pickle.dump(split_data, f)

print(f"Split saved at {save_path}")

import pickle

from google.colab import drive
drive.mount('/content/drive')

split_data_path = "/content/drive/MyDrive/archive2/split_preprocessed_data.pkl"
with open(split_data_path, "rb") as f:
    split_data = pickle.load(f)

print("Split dataset loaded successfully!")

import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt

G_train = nx.Graph()

train_ids = split_data["train"]["ids"]
train_texts = split_data["train"]["texts"]
train_labels = split_data["train"]["labels"]
train_features = split_data["train"]["features"]

for i, tweet_id in enumerate(train_ids):
    G_train.add_node(tweet_id, node_type="Tweet", label=train_labels[i], text=train_texts[i], features=train_features[i])

similarity_matrix = cosine_similarity(train_features)
threshold = 0.5

for i, id1 in enumerate(train_ids):
    for j, id2 in enumerate(train_ids):
        if i != j and similarity_matrix[i, j] > threshold:
            G_train.add_edge(id1, id2, weight=similarity_matrix[i, j], relation="text_similarity")

plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_train, seed=42, k=0.1)
nx.draw(G_train, pos, with_labels=False, node_size=10, node_color="blue", edge_color="gray", alpha=0.3)
plt.show()

print("Training graph visualization complete!")

!pip install torch torch-geometric

import torch
from torch_geometric.data import Data
from torch_geometric.utils import from_networkx
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity

def convert_to_pyg(graph):
    pyg_graph = from_networkx(graph)
    return pyg_graph

pyg_train = convert_to_pyg(G_train)

# Create and convert validation and test graphs similar to the training graph

# --- Create G_val ---
val_ids = split_data["val"]["ids"]
val_texts = split_data["val"]["texts"]
val_labels = split_data["val"]["labels"]
val_features = split_data["val"]["features"]

G_val = nx.Graph()  # Initialize the validation graph
for i, tweet_id in enumerate(val_ids):
    G_val.add_node(tweet_id, node_type="Tweet", label=val_labels[i], text=val_texts[i], features=val_features[i])

similarity_matrix_val = cosine_similarity(val_features)
threshold = 0.5
for i, id1 in enumerate(val_ids):
    for j, id2 in enumerate(val_ids):
        if i != j and similarity_matrix_val[i, j] > threshold:
            G_val.add_edge(id1, id2, weight=similarity_matrix_val[i, j], relation="text_similarity")
# --- End of Create G_val ---

pyg_val = convert_to_pyg(G_val)

# --- Create G_test ---
test_ids = split_data["test"]["ids"]
test_texts = split_data["test"]["texts"]
test_labels = split_data["test"]["labels"]
test_features = split_data["test"]["features"]

G_test = nx.Graph()  # Initialize the test graph
for i, tweet_id in enumerate(test_ids):
    G_test.add_node(tweet_id, node_type="Tweet", label=test_labels[i], text=test_texts[i], features=test_features[i])

similarity_matrix_test = cosine_similarity(test_features)
threshold = 0.5
for i, id1 in enumerate(test_ids):
    for j, id2 in enumerate(test_ids):
        if i != j and similarity_matrix_test[i, j] > threshold:
            G_test.add_edge(id1, id2, weight=similarity_matrix_test[i, j], relation="text_similarity")
# --- End of Create G_test ---

pyg_test = convert_to_pyg(G_test)


print("Graphs successfully converted for PyTorch Geometric!")

import torch
from torch_geometric.data import Data
from torch_geometric.utils import from_networkx

def convert_to_pyg_with_features(G, is_undirected=True):
    # Get node features and labels from the NetworkX graph
    node_features = []
    node_labels = []

    # Ensure consistent node ordering
    nodes = list(G.nodes())

    for node in nodes:
        node_features.append(G.nodes[node]['features'])
        node_labels.append(G.nodes[node]['label'])

    # Convert to torch tensors
    x = torch.FloatTensor(node_features)
    y = torch.LongTensor(node_labels)

    # Convert the NetworkX graph to PyG format
    pyg_graph = from_networkx(G)

    # Add the features and labels
    pyg_graph.x = x
    pyg_graph.y = y

    if is_undirected:
        pyg_graph.edge_index = torch.cat([pyg_graph.edge_index,
                                        pyg_graph.edge_index.flip(0)], dim=1)

    return pyg_graph

# Convert all three graphs
pyg_train = convert_to_pyg_with_features(G_train)
pyg_val = convert_to_pyg_with_features(G_val)
pyg_test = convert_to_pyg_with_features(G_test)

# Verify the conversion
print("Training Graph:")
print(f"Number of nodes: {pyg_train.num_nodes}")
print(f"Number of edges: {pyg_train.num_edges}")
print(f"Feature dimensions: {pyg_train.x.size()}")
print(f"Number of labels: {pyg_train.y.size(0)}")

# Now we can proceed with the model implementation
in_channels = pyg_train.x.size(1)  # Feature dimension from your TF-IDF
hidden_channels = 64
out_channels = 4  # Number of classes (true, false, unverified, non-rumor)
num_relations = 1  # Currently only using text_similarity relation

# Initialize model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = OptimizedRGCN(in_channels, hidden_channels, out_channels, num_relations).to(device)

print("\nModel created successfully!")

# Debug checks
print("Available keys in split_data:", split_data.keys())
print("\nAvailable keys in split_data['train']:", split_data['train'].keys())
print("\nSample of data shapes:")
for split in ['train', 'val', 'test']:
    print(f"\n{split.upper()} split:")
    for key, value in split_data[split].items():
        if isinstance(value, (list, np.ndarray)):
            print(f"{key}: {len(value)} items")

"""# PART 1"""

#OG FUNCTION- NEAR 65, BOTH

import torch
import torch.nn.functional as F
from torch_geometric.nn import RGCNConv
import torch.optim as optim
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, label_binarize
import sklearn.metrics as metrics
from itertools import cycle
import pandas as pd
import nltk
import transformers
import os
import warnings
warnings.filterwarnings('ignore')

# Download required NLTK data
try:
    nltk.download('punkt')
    nltk.download('averaged_perceptron_tagger')
    nltk.download('maxent_ne_chunker')
    nltk.download('words')
except:
    pass

# Constants
TIMESTAMP = "2025-03-24 18:10:32"
USER_LOGIN = "divsspace"

class OptimizedRGCN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels=128, out_channels=4, num_relations=1, num_bases=8):
        super().__init__()
        self.dropout_rate = 0.3

        # Enhanced main layers
        self.conv1 = RGCNConv(in_channels, hidden_channels, num_relations, num_bases=num_bases)
        self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations, num_bases=num_bases)
        self.conv3 = RGCNConv(hidden_channels, hidden_channels, num_relations, num_bases=num_bases)

        # Batch normalization layers
        self.batch_norm1 = torch.nn.BatchNorm1d(hidden_channels)
        self.batch_norm2 = torch.nn.BatchNorm1d(hidden_channels)
        self.batch_norm3 = torch.nn.BatchNorm1d(hidden_channels)

        # Skip connections
        self.skip1 = torch.nn.Linear(in_channels, hidden_channels)
        self.skip2 = torch.nn.Linear(hidden_channels, hidden_channels)

        # In Part 1, add this line after skip connections
        self.gate2 = torch.nn.Linear(hidden_channels, hidden_channels)  # Add this line in Part 1

        # Output layers
        self.linear1 = torch.nn.Linear(hidden_channels, hidden_channels // 2)
        self.linear2 = torch.nn.Linear(hidden_channels // 2, hidden_channels // 4)
        self.linear3 = torch.nn.Linear(hidden_channels // 4, out_channels)

        # Layer normalization
        self.layer_norm1 = torch.nn.LayerNorm(hidden_channels)
        self.layer_norm2 = torch.nn.LayerNorm(hidden_channels // 2)

        # Dropout
        self.dropout = torch.nn.Dropout(self.dropout_rate)

    def forward(self, x, edge_index, edge_type):
        # First conv block with skip connection
        identity1 = self.skip1(x)
        x = self.conv1(x, edge_index, edge_type)
        x = self.batch_norm1(x)
        x = F.elu(x)
        x = x + identity1
        x = self.layer_norm1(x)
        x = self.dropout(x)

        # Second conv block with skip connection
        identity2 = self.skip2(x)
        x = self.conv2(x, edge_index, edge_type)
        x = self.batch_norm2(x)
        x = F.elu(x)
        # x = x + identity2
        g2 = torch.sigmoid(self.gate2(x)) # Line 68 replacement
        x = g2 * x + (1 - g2) * identity2 # Line 68 new
        x = self.layer_norm1(x)
        x = self.dropout(x)

        # Third conv block
        x = self.conv3(x, edge_index, edge_type)
        x = self.batch_norm3(x)
        x = F.elu(x)
        x = self.dropout(x)

        # Dense layers
        x = self.linear1(x)
        x = F.elu(x)
        x = self.layer_norm2(x)
        x = self.dropout(x)

        x = self.linear2(x)
        x = F.elu(x)
        x = self.dropout(x)

        x = self.linear3(x)

        return F.log_softmax(x, dim=1)

class OptimizedConfig:
    def __init__(self):
        # Core hyperparameters
        self.LEARNING_RATE = 0.0005
        self.WEIGHT_DECAY = 5e-4
        self.HIDDEN_CHANNELS = 128
        self.EPOCHS = 300
        self.EARLY_STOPPING_PATIENCE = 25
        self.DROPOUT = 0.3
        self.NUM_BASES = 8
        self.GRADIENT_CLIP = 0.5
        self.BATCH_SIZE = 32

        # Data augmentation parameters
        self.AUGMENTATION_PROBABILITY = 0.15
        self.MIXUP_ALPHA = 0.2

        # Learning rate scheduling
        self.LR_REDUCE_FACTOR = 0.5
        self.LR_PATIENCE = 10
        self.MIN_LR = 1e-6
        self.WARMUP_EPOCHS = 15

        # Model specific
        self.NUM_LAYERS = 3
        self.RESIDUAL = True
        self.USE_LAYER_NORM = True

        # Dataset specific parameters (from data analysis)
        self.NUM_CLASSES = 4        # From data analysis (labels 0-3)
        self.FEATURE_DIM = 500      # From feature dimensions
        self.NUM_RELATIONS = 5      # For edge types in graph structure
        self.NUM_NODES_TRAIN = 1496 # From training set size
        self.NUM_NODES_VAL = 321    # From validation set size
        self.NUM_NODES_TEST = 321   # From test set size

        # Current metadata
        self.TIMESTAMP = "2025-03-24 19:07:24"
        self.USER_LOGIN = "divsspace"

    def __str__(self):
        """Returns a formatted string of all configuration parameters"""
        return "\n".join(f"{key}: {value}" for key, value in vars(self).items())

"""# MODEL

# PART 2

NEW PT 2.
"""

# Part 2: Training History and Visualization Components

import torch
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import matplotlib.pyplot as plt
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class SmallDatasetFeatureProcessor:
    def __init__(self):
        """Initialize with current timestamp: 2025-03-24 20:05:38"""
        self.scaler = StandardScaler()
        self.minmax = MinMaxScaler(feature_range=(-1, 1))
        self.timestamp = "2025-03-24 20:05:38"
        self.user_login = "divsspace"

        # Dataset specific parameters from data analysis
        self.feature_dim = 500
        self.num_classes = 4
        self.train_size = 1496
        self.val_size = 321
        self.test_size = 321

    def enhance_features(self, features, texts):
        """
        Process pre-computed 500-dimensional features

        Args:
            features: numpy array of shape (n_samples, 500)
            texts: list of texts (not used as features are pre-computed)

        Returns:
            torch.FloatTensor: Normalized features
        """
        print("Processing features...")

        # Convert to numpy array if not already
        features = np.array(features)

        # Handle NaN and Inf values
        features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)

        # Normalize features
        features = self.scaler.fit_transform(features)
        features = self.minmax.fit_transform(features)

        return torch.FloatTensor(features)

    def get_feature_stats(self):
        """Return current feature statistics"""
        if hasattr(self.scaler, 'mean_'):
            return {
                'mean': self.scaler.mean_,
                'scale': self.scaler.scale_,
                'min': self.minmax.min_,
                'max': self.minmax.max_,
                'feature_dim': self.feature_dim,
                'timestamp': self.timestamp
            }
        return None


class TrainingHistory:
    def __init__(self):
        """Initialize with current timestamp: 2025-03-24 20:05:38"""
        self.timestamp = "2025-03-24 20:05:38"
        self.user_login = "divsspace"
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []
        self.learning_rates = []
        self.epochs = []
        self.best_epoch = 0
        self.best_val_acc = 0

    def update(self, epoch, train_loss, val_loss, train_acc, val_acc, lr):
        """Update training history with new metrics"""
        self.epochs.append(epoch)
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        self.train_accs.append(train_acc)
        self.val_accs.append(val_acc)
        self.learning_rates.append(lr)

        if val_acc > self.best_val_acc:
            self.best_val_acc = val_acc
            self.best_epoch = epoch

    def plot_metrics(self):
        """Plot training metrics with updated style"""
        plt.style.use('default')  # Changed from seaborn to default
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # Loss plot
        ax1.plot(self.epochs, self.train_losses, label='Train Loss', color='blue', alpha=0.7)
        ax1.plot(self.epochs, self.val_losses, label='Validation Loss', color='red', alpha=0.7)
        ax1.set_title('Loss over epochs')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)

        # Accuracy plot
        ax2.plot(self.epochs, self.train_accs, label='Train Accuracy', color='blue', alpha=0.7)
        ax2.plot(self.epochs, self.val_accs, label='Validation Accuracy', color='red', alpha=0.7)
        ax2.set_title('Accuracy over epochs')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)

        # Learning rate plot
        ax3.plot(self.epochs, self.learning_rates, color='green', alpha=0.7)
        ax3.set_title('Learning Rate over epochs')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Learning Rate')
        ax3.set_yscale('log')
        ax3.grid(True)

        # Training progress
        window = 5
        val_acc_smoothed = np.convolve(self.val_accs,
                                     np.ones(window)/window,
                                     mode='valid')
        ax4.plot(self.epochs[window-1:], val_acc_smoothed,
                color='purple', alpha=0.7)
        ax4.set_title(f'Smoothed Validation Accuracy (window={window})')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Accuracy')
        ax4.grid(True)

        plt.tight_layout()

        # Save plot
        plt.savefig(f'training_history_{self.timestamp.replace(" ", "_").replace(":", "-")}.png')
        plt.close()  # Close the plot to free memory

        # Print summary
        print(f"\nTraining Summary (as of {self.timestamp}):")
        print(f"User: {self.user_login}")
        print(f"Best Validation Accuracy: {self.best_val_acc:.4f} at epoch {self.best_epoch}")
        print(f"Final Training Accuracy: {self.train_accs[-1]:.4f}")
        print(f"Final Validation Accuracy: {self.val_accs[-1]:.4f}")
        print(f"Total Epochs: {len(self.epochs)}")
        print(f"Final Learning Rate: {self.learning_rates[-1]:.6f}")

    def save_history(self, path=None):
        """Save training history to file"""
        history_data = {
            'timestamp': self.timestamp,
            'user_login': self.user_login,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accs': self.train_accs,
            'val_accs': self.val_accs,
            'learning_rates': self.learning_rates,
            'epochs': self.epochs,
            'best_epoch': self.best_epoch,
            'best_val_acc': self.best_val_acc
        }

        if path is None:
            path = f'training_history_{self.timestamp.replace(" ", "_").replace(":", "-")}.pt'

        torch.save(history_data, path)
        print(f"Training history saved to {path}")

"""# PART 3

PT 3 NEW
"""

import torch
import torch.nn.functional as F
import numpy as np
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class SmallDatasetTrainer:
    def __init__(self, model, config, device, timestamp, user_login):
        print(f"\nInitializing trainer at {timestamp}")
        print(f"Using device: {device}")

        self.model = model
        self.config = config
        self.device = device
        self.timestamp = timestamp
        self.user_login = user_login
        self.history = TrainingHistory()

        print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")

    def mixup_data(self, x, y, alpha=0.2):
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1

        batch_size = x.size()[0]
        index = torch.randperm(batch_size)

        mixed_x = lam * x + (1 - lam) * x[index]
        y_a, y_b = y, y[index]
        return mixed_x, y_a, y_b, lam

    def mixup_criterion(self, criterion, pred, y_a, y_b, lam):
        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

    def label_smoothing_loss(self, pred, target, epsilon=0.1):
        n_classes = pred.size(1)
        target_one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)
        smoothed_target = (1 - epsilon) * target_one_hot + epsilon / n_classes
        return torch.mean(torch.sum(-smoothed_target * pred, dim=1))

    def train_epoch(self, data, optimizer, epoch):
        self.model.train()
        total_loss = 0
        total_correct = 0
        num_samples = 0

        x = data['x']
        edge_index = data['edge_index']
        edge_type = data['edge_type']
        y = data['labels']

        batch_size = min(32, len(x))  # Use smaller batch size
        num_batches = (len(x) + batch_size - 1) // batch_size

        for i in range(0, len(x), batch_size):
            batch_end = min(i + batch_size, len(x))
            batch_idx = torch.arange(i, batch_end)

            # Get batch data
            batch_x = x[batch_idx]
            batch_y = y[batch_idx]

            # Get batch subgraph
            batch_mask = torch.zeros(len(x), dtype=torch.bool)
            batch_mask[batch_idx] = True

            # Get edges within batch
            edge_mask = batch_mask[edge_index[0]] & batch_mask[edge_index[1]]
            batch_edge_index = edge_index[:, edge_mask]
            batch_edge_type = edge_type[edge_mask]

            # Remap node indices to batch
            node_idx = torch.zeros(len(x), dtype=torch.long)
            node_idx[batch_idx] = torch.arange(len(batch_idx))
            batch_edge_index = node_idx[batch_edge_index]

            # Apply mixup
            mixed_x, y_a, y_b, lam = self.mixup_data(batch_x, batch_y)

            # Forward pass
            optimizer.zero_grad()
            out = self.model(
                mixed_x.to(self.device),
                batch_edge_index.to(self.device),
                batch_edge_type.to(self.device)
            )

            # Compute loss
            loss = self.mixup_criterion(
                F.cross_entropy,
                out,
                y_a.to(self.device),
                y_b.to(self.device),
                lam
            )

            # Backward pass
            loss.backward()
            optimizer.step()

            # Update metrics
            pred = out.argmax(dim=1)
            correct = (pred == batch_y.to(self.device)).sum().item()

            total_loss += loss.item() * len(batch_idx)
            total_correct += correct
            num_samples += len(batch_idx)

            if i % (10 * batch_size) == 0:
                print(f"Batch {i//batch_size}/{num_batches}, Loss: {loss.item():.4f}")

        return total_loss / num_samples, total_correct / num_samples

    def validate(self, data):
        self.model.eval()
        total_loss = 0
        total_correct = 0

        x = data['x']
        edge_index = data['edge_index']
        edge_type = data['edge_type']
        y = data['labels']

        batch_size = min(32, len(x))
        uncertainties = []

        with torch.no_grad():
            for i in range(0, len(x), batch_size):
                batch_end = min(i + batch_size, len(x))
                batch_idx = torch.arange(i, batch_end)

                # Get batch data
                batch_x = x[batch_idx]
                batch_y = y[batch_idx]

                # Get batch subgraph
                batch_mask = torch.zeros(len(x), dtype=torch.bool)
                batch_mask[batch_idx] = True

                # Get edges within batch
                edge_mask = batch_mask[edge_index[0]] & batch_mask[edge_index[1]]
                batch_edge_index = edge_index[:, edge_mask]
                batch_edge_type = edge_type[edge_mask]

                # Remap node indices to batch
                node_idx = torch.zeros(len(x), dtype=torch.long)
                node_idx[batch_idx] = torch.arange(len(batch_idx))
                batch_edge_index = node_idx[batch_edge_index]

                # Forward pass
                out = self.model(
                    batch_x.to(self.device),
                    batch_edge_index.to(self.device),
                    batch_edge_type.to(self.device)
                )

                # Calculate loss and metrics
                loss = F.cross_entropy(out, batch_y.to(self.device))
                pred = out.argmax(dim=1)
                correct = (pred == batch_y.to(self.device)).sum().item()

                # Calculate uncertainty
                probs = torch.exp(out)
                uncertainty = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)
                uncertainties.append(uncertainty)

                total_loss += loss.item() * len(batch_idx)
                total_correct += correct

        avg_loss = total_loss / len(x)
        accuracy = total_correct / len(x)
        avg_uncertainty = torch.cat(uncertainties).mean().item()

        return avg_loss, accuracy, avg_uncertainty

    def save_checkpoint(self, epoch, optimizer, val_acc, uncertainty):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc,
            'uncertainty': uncertainty
        }

        checkpoint_path = f'checkpoint_epoch{epoch}_acc{val_acc:.4f}_{self.timestamp.replace(" ", "_").replace(":", "-")}.pt'
        torch.save(checkpoint, checkpoint_path)
        return checkpoint_path

    def load_checkpoint(self, checkpoint_path):
        checkpoint = torch.load(checkpoint_path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        return checkpoint['epoch'], checkpoint['val_acc']

"""# PART 4

PART 4- NEW
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
from sklearn import metrics
import warnings
warnings.filterwarnings('ignore')

def comprehensive_testing(model, test_data, device, timestamp="2025-03-24 19:56:00", user_login="divsspace"):
    """
    Comprehensive testing framework optimized for pre-processed features
    with uncertainty estimation and error analysis.
    """
    print(f"\n=== Starting Comprehensive Testing ===")
    print(f"Timestamp: {timestamp}")
    print(f"User: {user_login}")

    model.eval()  # Keep model in eval mode throughout testing
    test_results = {}

    # 1. Multi-pass Testing
    with torch.no_grad():
        n_passes = 10
        all_outputs = []
        all_uncertainties = []

        x = test_data['x'].to(device)
        edge_index = test_data['edge_index'].to(device)
        edge_type = test_data['edge_type'].to(device)
        y = test_data['labels'].to(device)
        num_samples = len(y)

        print("\nPerforming multiple forward passes...")

        # Process in batches with minimum batch size of 4
        batch_size = max(4, min(32, len(x) // 10))
        num_batches = (len(x) + batch_size - 1) // batch_size

        for i in range(n_passes):
            pass_outputs = torch.zeros((num_samples, 4), device=device)  # 4 classes
            pass_uncertainties = torch.zeros(num_samples, device=device)

            # Enable dropout but keep batch norm in eval mode
            model.train()
            for m in model.modules():
                if isinstance(m, torch.nn.BatchNorm1d):
                    m.eval()

            for j in range(num_batches):
                start_idx = j * batch_size
                end_idx = min((j + 1) * batch_size, len(x))

                # Ensure minimum batch size
                actual_start = start_idx
                if end_idx - start_idx < 4:
                    actual_start = max(0, end_idx - 4)

                batch_idx = torch.arange(actual_start, end_idx)

                # Get batch data
                batch_x = x[batch_idx]

                # Get batch subgraph
                mask = torch.zeros(len(x), dtype=torch.bool, device=device)
                mask[batch_idx] = True
                edge_mask = mask[edge_index[0]] & mask[edge_index[1]]
                batch_edge_index = edge_index[:, edge_mask]
                batch_edge_type = edge_type[edge_mask]

                # Remap node indices
                node_idx = torch.zeros(len(x), dtype=torch.long, device=device)
                node_idx[batch_idx] = torch.arange(len(batch_idx), device=device)
                batch_edge_index = node_idx[batch_edge_index]

                # Forward pass
                outputs = model(batch_x, batch_edge_index, batch_edge_type)
                probs = torch.exp(outputs)

                # Calculate uncertainty
                uncertainty = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)

                # Store only the relevant outputs (handling minimum batch size padding)
                valid_indices = slice(start_idx - actual_start, end_idx - actual_start)
                pass_outputs[start_idx:end_idx] = probs[valid_indices]
                pass_uncertainties[start_idx:end_idx] = uncertainty[valid_indices]

            all_outputs.append(pass_outputs)
            all_uncertainties.append(pass_uncertainties)

            if (i + 1) % 2 == 0:
                print(f"Completed {i + 1}/{n_passes} passes")

        # Switch back to eval mode
        model.eval()

        # Aggregate predictions
        avg_output = torch.mean(torch.stack(all_outputs), dim=0)
        avg_uncertainty = torch.mean(torch.stack(all_uncertainties), dim=0)

        # Get predictions
        preds = avg_output.argmax(dim=1)
        true_labels = y.cpu().numpy()
        pred_labels = preds.cpu().numpy()
        probs_np = avg_output.cpu().numpy()
        uncertainties_np = avg_uncertainty.cpu().numpy()

        # Calculate metrics
        accuracy = (preds == y).float().mean().item()

        # Per-class metrics
        n_classes = 4  # From data analysis
        per_class_metrics = []

        print("\nCalculating per-class metrics...")
        for i in range(n_classes):
            class_mask = true_labels == i
            if np.any(class_mask):
                class_metrics = {
                    'accuracy': (pred_labels[class_mask] == true_labels[class_mask]).mean(),
                    'precision': precision_score(true_labels == i, pred_labels == i),
                    'recall': recall_score(true_labels == i, pred_labels == i),
                    'f1': f1_score(true_labels == i, pred_labels == i),
                    'avg_confidence': probs_np[class_mask, i].mean(),
                    'avg_uncertainty': uncertainties_np[class_mask].mean(),
                    'support': np.sum(class_mask)
                }
                per_class_metrics.append(class_metrics)

        # 2. Error Analysis
        print("\nPerforming error analysis...")
        error_mask = pred_labels != true_labels
        error_indices = np.where(error_mask)[0]

        error_analysis = {
            'indices': error_indices,
            'true_labels': true_labels[error_indices],
            'predicted_labels': pred_labels[error_indices],
            'confidences': probs_np[error_indices].max(axis=1),
            'uncertainties': uncertainties_np[error_indices],
            'error_rate': len(error_indices) / len(true_labels)
        }

        # 3. Calibration Analysis
        print("\nAnalyzing model calibration...")
        confidence_bins = np.linspace(0, 1, 11)
        calibration_stats = []

        for i in range(len(confidence_bins) - 1):
            bin_mask = (probs_np.max(axis=1) >= confidence_bins[i]) & \
                      (probs_np.max(axis=1) < confidence_bins[i + 1])
            if np.any(bin_mask):
                bin_acc = (pred_labels[bin_mask] == true_labels[bin_mask]).mean()
                bin_conf = probs_np[bin_mask].max(axis=1).mean()
                calibration_stats.append({
                    'bin_range': (confidence_bins[i], confidence_bins[i + 1]),
                    'accuracy': bin_acc,
                    'confidence': bin_conf,
                    'samples': bin_mask.sum()
                })

        # Store results
        test_results = {
            'timestamp': timestamp,
            'user': user_login,
            'overall_accuracy': accuracy,
            'per_class_metrics': per_class_metrics,
            'confusion_matrix': confusion_matrix(true_labels, pred_labels),
            'error_analysis': error_analysis,
            'calibration_stats': calibration_stats,
            'predictions': pred_labels,
            'probabilities': probs_np,
            'uncertainties': uncertainties_np,
            'true_labels': true_labels
        }

        # Visualization with default style
        print("\nGenerating visualizations...")
        plt.style.use('default')

        # 1. Confusion Matrix
        plt.figure(figsize=(10, 8))
        plt.imshow(test_results['confusion_matrix'], cmap='Blues', interpolation='nearest')
        plt.colorbar()
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        for i in range(n_classes):
            for j in range(n_classes):
                plt.text(j, i, str(test_results['confusion_matrix'][i, j]),
                        ha='center', va='center')
        plt.savefig(f'confusion_matrix_{timestamp.replace(" ", "_").replace(":", "-")}.png')
        plt.close()

        # 2. Confidence vs Accuracy
        plt.figure(figsize=(10, 5))
        accuracies = [stats['accuracy'] for stats in calibration_stats]
        confidences = [stats['confidence'] for stats in calibration_stats]
        plt.plot(confidences, accuracies, 'b-', label='Model')
        plt.plot([0, 1], [0, 1], 'r--', label='Perfect Calibration')
        plt.xlabel('Confidence')
        plt.ylabel('Accuracy')
        plt.title('Reliability Diagram')
        plt.legend()
        plt.grid(True)
        plt.savefig(f'reliability_diagram_{timestamp.replace(" ", "_").replace(":", "-")}.png')
        plt.close()

        # 3. Uncertainty Distribution
        plt.figure(figsize=(10, 5))
        plt.hist(uncertainties_np[~error_mask], bins=30, alpha=0.5, label='Correct', density=True)
        plt.hist(uncertainties_np[error_mask], bins=30, alpha=0.5, label='Incorrect', density=True)
        plt.xlabel('Uncertainty')
        plt.ylabel('Density')
        plt.title('Uncertainty Distribution for Correct vs Incorrect Predictions')
        plt.legend()
        plt.grid(True)
        plt.savefig(f'uncertainty_dist_{timestamp.replace(" ", "_").replace(":", "-")}.png')
        plt.close()

        # Print summary
        print("\nTest Results Summary:")
        print(f"Overall Accuracy: {accuracy:.4f}")
        print(f"Error Rate: {error_analysis['error_rate']:.4f}")
        print("\nPer-class Performance:")
        for i, metrics_dict in enumerate(per_class_metrics):
            print(f"\nClass {i}:")
            print(f"Support: {metrics_dict['support']} samples")
            print(f"Accuracy: {metrics_dict['accuracy']:.4f}")
            print(f"F1-Score: {metrics_dict['f1']:.4f}")
            print(f"Average Confidence: {metrics_dict['avg_confidence']:.4f}")
            print(f"Average Uncertainty: {metrics_dict['avg_uncertainty']:.4f}")

        # Save results
        results_path = f'test_results_{timestamp.replace(" ", "_").replace(":", "-")}.pt'
        torch.save(test_results, results_path)
        print(f"\nDetailed results saved to: {results_path}")

        return test_results

"""# PART 5

PART 5 NEW
"""

from datetime import datetime
import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
try:
    import seaborn as sns
    plt.style.use('seaborn-v0_8')  # Using updated seaborn style name
except:
    plt.style.use('default')  # Fallback to default style if seaborn fails
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
from sklearn.neighbors import kneighbors_graph
import warnings
warnings.filterwarnings('ignore')

if __name__ == "__main__":
    try:
        # Setup and Configuration
        print(f"Starting execution at: 2025-03-24 19:31:28")
        print(f"User: divsspace")

        # Set random seeds for reproducibility
        torch.manual_seed(42)
        torch.cuda.manual_seed_all(42)
        np.random.seed(42)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

        # Initialize device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"\nUsing device: {device}")

        # Initialize components
        config = OptimizedConfig()
        feature_processor = SmallDatasetFeatureProcessor()

        # Print data structure verification
        print("\nVerifying data structure...")
        print("Available keys in split_data:", split_data.keys())
        print("\nAvailable keys in split_data['train']:", split_data['train'].keys())
        print("\nSample of data shapes:")
        for split in ['train', 'val', 'test']:
            print(f"\n{split.upper()} split:")
            for key, value in split_data[split].items():
                if isinstance(value, (list, np.ndarray)):
                    print(f"{key}: {len(value)} items")

        # Process features
        print("\nProcessing features...")
        print("Processing train features...")
        train_features = feature_processor.enhance_features(
            split_data['train']['features'],
            split_data['train']['texts']
        )

        print("Processing validation features...")
        val_features = feature_processor.enhance_features(
            split_data['val']['features'],
            split_data['val']['texts']
        )

        print("Processing test features...")
        test_features = feature_processor.enhance_features(
            split_data['test']['features'],
            split_data['test']['texts']
        )

        print(f"Feature dimensions: {train_features.shape[1]}")

        # Convert features to torch tensors
        train_features = torch.FloatTensor(train_features)
        val_features = torch.FloatTensor(val_features)
        test_features = torch.FloatTensor(test_features)

        # Create graph structure
        print("\nCreating graph structure...")

        def create_graph_structure(features, k=5):
            """Create graph structure using k-nearest neighbors"""
            print(f"Creating graph with {k} nearest neighbors...")
            # Create adjacency matrix using k-nearest neighbors
            A = kneighbors_graph(
                features.numpy(),
                n_neighbors=k,
                mode='distance',
                include_self=False
            )
            A = A.tocoo()

            # Create edge index
            edge_index = torch.tensor(np.vstack((A.row, A.col)), dtype=torch.long)

            # Create edge types based on distance quantiles
            distances = A.data
            num_edge_types = 5  # Number of edge types
            bins = np.linspace(distances.min(), distances.max(), num_edge_types)
            edge_type = torch.tensor(
                np.digitize(distances, bins=bins),
                dtype=torch.long
            ) - 1

            print(f"Created graph with {edge_index.shape[1]} edges")
            print(f"Edge type distribution: {torch.bincount(edge_type)}")

            return edge_index, edge_type

        # Create graph structures for each split
        print("\nCreating train graph...")
        train_edge_index, train_edge_type = create_graph_structure(train_features)

        print("\nCreating validation graph...")
        val_edge_index, val_edge_type = create_graph_structure(val_features)

        print("\nCreating test graph...")
        test_edge_index, test_edge_type = create_graph_structure(test_features)

        # Initialize model
        print("\nInitializing model...")
        model = OptimizedRGCN(
            in_channels=500,  # Fixed dimension from data
            hidden_channels=config.HIDDEN_CHANNELS,
            out_channels=4,   # Number of classes
            num_relations=5,  # Edge types
            num_bases=config.NUM_BASES
        ).to(device)

        print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")

        # Initialize optimizer
        optimizer = optim.AdamW(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY,
            betas=(0.9, 0.999)
        )

        # Initialize scheduler
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='max',
            factor=config.LR_REDUCE_FACTOR,
            patience=config.LR_PATIENCE,
            min_lr=config.MIN_LR,
            verbose=True
        )

        # Initialize trainer with updated batch processing
        trainer = SmallDatasetTrainer(
            model=model,
            config=config,
            device=device,
            timestamp="2025-03-24 19:31:28",
            user_login="divsspace"
        )

        # Training loop
        print("\nStarting training...")
        best_val_acc = 0
        early_stopping_counter = 0
        training_start_time = datetime.now()

        # Convert labels to tensors
        train_labels = torch.LongTensor(split_data['train']['labels'])
        val_labels = torch.LongTensor(split_data['val']['labels'])
        test_labels = torch.LongTensor(split_data['test']['labels'])

        for epoch in range(config.EPOCHS):
            epoch_start_time = datetime.now()

            # Train
            train_loss, train_acc = trainer.train_epoch(
                data={
                    'x': train_features,
                    'edge_index': train_edge_index,
                    'edge_type': train_edge_type,
                    'labels': train_labels
                },
                optimizer=optimizer,
                epoch=epoch
            )

            # Validate
            val_loss, val_acc, val_uncertainty = trainer.validate({
                'x': val_features,
                'edge_index': val_edge_index,
                'edge_type': val_edge_type,
                'labels': val_labels
            })

            # Update learning rate
            scheduler.step(val_acc)
            current_lr = optimizer.param_groups[0]['lr']

            # Update history
            trainer.history.update(
                epoch=epoch,
                train_loss=train_loss,
                val_loss=val_loss,
                train_acc=train_acc,
                val_acc=val_acc,
                lr=current_lr
            )

            # Calculate epoch time
            epoch_time = datetime.now() - epoch_start_time

            # Print progress
            print(f"\nEpoch {epoch:03d}:")
            print(f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
            print(f"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")
            print(f"Uncertainty: {val_uncertainty:.4f}, LR: {current_lr:.6f}")
            print(f"Time: {epoch_time}")

            # Save best model
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                early_stopping_counter = 0
                checkpoint_path = trainer.save_checkpoint(
                    epoch=epoch,
                    optimizer=optimizer,
                    val_acc=val_acc,
                    uncertainty=val_uncertainty
                )
                print(f"Saved checkpoint: {checkpoint_path}")
            else:
                early_stopping_counter += 1

            if early_stopping_counter >= config.EARLY_STOPPING_PATIENCE:
                print(f"\nEarly stopping triggered at epoch {epoch}")
                break

        training_time = datetime.now() - training_start_time
        print(f"\nTotal training time: {training_time}")

        # Test
        print("\nPerforming comprehensive testing...")
        test_results = comprehensive_testing(
            model=model,
            test_data={
                'x': test_features,
                'edge_index': test_edge_index,
                'edge_type': test_edge_type,
                'labels': test_labels
            },
            device=device,
            timestamp="2025-03-24 19:31:28",
            user_login="divsspace"
        )

        # Plot training history
        trainer.history.plot_metrics()

        # Save final results
        final_results = {
            'timestamp': "2025-03-24 19:31:28",
            'user': "divsspace",
            'train_accuracy': train_acc,
            'val_accuracy': val_acc,
            'test_accuracy': test_results['overall_accuracy'],
            'training_time': str(training_time),
            'best_epoch': trainer.history.best_epoch,
            'best_val_acc': trainer.history.best_val_acc,
            'feature_dimensions': train_features.shape[1],
            'model_config': vars(config),
            'test_results': test_results,
            'device_used': str(device),
            'graph_stats': {
                'train_edges': train_edge_index.shape[1],
                'val_edges': val_edge_index.shape[1],
                'test_edges': test_edge_index.shape[1],
                'edge_type_distribution': {
                    'train': torch.bincount(train_edge_type).tolist(),
                    'val': torch.bincount(val_edge_type).tolist(),
                    'test': torch.bincount(test_edge_type).tolist()
                }
            }
        }

        final_results_path = '/content/drive/MyDrive/archive2/final_results_2025-03-24_19-31-28.pt'
        torch.save(final_results, final_results_path)
        print(f"\nFinal results saved to: {final_results_path}")

        print("\nExecution completed successfully!")

    except Exception as e:
        print(f"\nError occurred: {str(e)}")
        print("\nFull traceback:")
        import traceback
        traceback.print_exc()